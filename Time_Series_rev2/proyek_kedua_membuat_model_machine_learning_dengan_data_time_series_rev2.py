# -*- coding: utf-8 -*-
"""Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series rev2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1owkb090p4vxdlmpfLiCmGULfswJPP5Yl

# Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series

---


# Daniel Shandy Adryan
# 1494037162101-932
# danielshandy34@gmail.com

# Preparation

Importing Libraries and API
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM, Dropout, BatchNormalization,Bidirectional
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf

"""Declaring variables to simplify the source code"""

rc=pd.read_csv
bn=BatchNormalization
ss=MinMaxScaler()
tts=train_test_split

"""Reading the csv-files and fill the NaN data"""

df = rc('/content/time-series-19-covid-combined.csv', usecols=['Date', 'Confirmed', 'Recovered', 'Deaths'])

df.tail()

"""Fill the missing data"""

df.fillna(method='ffill', inplace=True)
df.isnull().sum()

train_dates = pd.to_datetime(df['Date'])
print(train_dates.tail(15))

"""Plotting the dataframe"""

dt = df ['Date'].values
dikonfirmasi = df['Confirmed'].values
 
plt.figure(figsize=(20,5))
plt.plot(dt, dikonfirmasi)
plt.title('Confirmed Daily Cases');

"""Normalizing the data"""

mm_scaler = ss.fit_transform(dikonfirmasi.reshape(-1,1))
dk_scaled = mm_scaler.reshape(-1)

"""Calculating the Threshold for mae"""

threshold = (dk_scaled.max() - dk_scaled.min()) * 10/100
threshold

"""# Modelling and Training

define the function for creating datasets datasets
"""

dt_train, dt_test, dk_train, dk_test = tts(dt, dk_scaled, test_size=0.2, shuffle=False)

def wds(ser, win_size, batch_size, shuffle_buffer):
    ser = tf.expand_dims(ser, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(ser)
    ds = ds.window(win_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(win_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

"""Modelling"""

traindata = wds(dk_train, win_size=70, batch_size=10, shuffle_buffer=1000)
vd = wds(dk_test, win_size=70, batch_size=10, shuffle_buffer=1000)
model = tf.keras.models.Sequential()
model.add(LSTM(70, return_sequences=True))
model.add(LSTM(70))
model.add(Dense(32, activation='relu'))
model.add(bn())
model.add(Dropout(0.1))
model.add(Dense(16, activation='relu'))
model.add(bn())
model.add(Dropout(0.1))
model.add(Dense(1))

"""Printing Number of samples in a datasets"""

print('train sample :\n', len(list(traindata)))
print('val sample :\n', len(list(vd)))

"""Training"""

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
train = model.fit(traindata, steps_per_epoch=20, epochs=60, validation_data=vd)

"""# Plotting

Loss Plotting
"""

plt.plot(train.history['loss'], label = 'Latih')
plt.plot(train.history['val_loss'], label = 'Test')
plt.legend()

"""mae Plotting"""

plt.plot(train.history['mae'], label = 'Latih')
plt.plot(train.history['val_mae'], label = 'Test')
plt.legend()